{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Women's Health Data Analysis and Visualization - Part 3\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook is the third in a series focused on creating a dataset for training an LLM model to predict better questions for women's health consultations. This part focuses on in-depth analysis and visualization of the preprocessed data from Part 2.\n",
    "\n",
    "### Objectives\n",
    "- Load the preprocessed data from Part 2\n",
    "- Perform comprehensive analysis of women's health questions\n",
    "- Create detailed visualizations to identify patterns\n",
    "- Analyze dismissal patterns across different demographics\n",
    "- Identify key factors that contribute to question effectiveness\n",
    "- Prepare insights for LLM training\n",
    "\n",
    "### Why This Matters\n",
    "Understanding the patterns in women's health questions and dismissal experiences is crucial for training an effective LLM model. By visualizing these patterns, we can identify the key characteristics that make questions more effective and less likely to be dismissed by healthcare providers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's set up our environment by importing necessary libraries and loading the preprocessed data from Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set up color palettes\n",
    "category_palette = sns.color_palette(\"viridis\", 10)\n",
    "dismissal_palette = {'Very High': '#d62728', 'High': '#ff7f0e', 'Medium': '#ffbb78', 'Low': '#2ca02c'}\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Display versions for reproducibility\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"NLTK version: {nltk.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")\n",
    "print(f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory structure\n",
    "data_dir = 'womens_health_data'\n",
    "raw_dir = os.path.join(data_dir, 'raw')\n",
    "processed_dir = os.path.join(data_dir, 'processed')\n",
    "expanded_dir = os.path.join(data_dir, 'expanded')\n",
    "checkpoint_dir = os.path.join(data_dir, 'checkpoints')\n",
    "figures_dir = os.path.join(data_dir, 'figures')\n",
    "analysis_dir = os.path.join(data_dir, 'analysis')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [data_dir, raw_dir, processed_dir, expanded_dir, checkpoint_dir, figures_dir, analysis_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"Created directory: {directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "Let's create some helper functions for data analysis, visualization, and checkpoint management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(df, name):\n",
    "    \"\"\"\n",
    "    Save a dataframe as a checkpoint CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame to save\n",
    "    - name: name of the checkpoint (without extension)\n",
    "    \n",
    "    Returns:\n",
    "    - path: path to the saved file\n",
    "    \"\"\"\n",
    "    # Create the full path with timestamp to avoid overwriting\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{name}_{timestamp}.csv\"\n",
    "    path = os.path.join(checkpoint_dir, filename)\n",
    "    \n",
    "    # Save the dataframe\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Checkpoint saved: {path}\")\n",
    "    \n",
    "    # Also save a version with a fixed name for easy loading\n",
    "    fixed_path = os.path.join(checkpoint_dir, f\"{name}_latest.csv\")\n",
    "    df.to_csv(fixed_path, index=False)\n",
    "    print(f\"Latest version saved: {fixed_path}\")\n",
    "    \n",
    "    return path\n",
    "\n",
    "def load_checkpoint(name):\n",
    "    \"\"\"\n",
    "    Load the latest checkpoint for a given name.\n",
    "    \n",
    "    Parameters:\n",
    "    - name: name of the checkpoint (without extension)\n",
    "    \n",
    "    Returns:\n",
    "    - df: loaded DataFrame or None if file doesn't exist\n",
    "    \"\"\"\n",
    "    path = os.path.join(checkpoint_dir, f\"{name}_latest.csv\")\n",
    "    \n",
    "    if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            print(f\"Checkpoint loaded: {path}\")\n",
    "            print(f\"Shape: {df.shape}\")\n",
    "            return df\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Warning: Checkpoint file exists but is empty: {path}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Checkpoint not found or empty: {path}\")\n",
    "        return None\n",
    "\n",
    "def verify_dataframe(df, name):\n",
    "    \"\"\"\n",
    "    Verify a dataframe by displaying basic information.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame to verify\n",
    "    - name: name of the dataframe for display purposes\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- {name} Verification ---\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(df.head())\n",
    "    print(\"\\nData types:\")\n",
    "    display(df.dtypes)\n",
    "    print(\"\\nMissing values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    display(missing[missing > 0] if any(missing > 0) else \"No missing values\")\n",
    "    print(\"\\nBasic statistics:\")\n",
    "    display(df.describe(include='all').T)\n",
    "    print(\"----------------------------\\n\")\n",
    "\n",
    "def create_wordcloud(text, title, filename, mask=None, background_color='white', colormap='viridis', max_words=200):\n",
    "    \"\"\"\n",
    "    Create and save a word cloud visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: text to visualize\n",
    "    - title: title for the plot\n",
    "    - filename: filename to save the plot (without extension)\n",
    "    - mask: optional mask image for the word cloud\n",
    "    - background_color: background color for the word cloud\n",
    "    - colormap: colormap for the word cloud\n",
    "    - max_words: maximum number of words to include\n",
    "    \"\"\"\n",
    "    # Create the word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                          background_color=background_color,\n",
    "                          colormap=colormap,\n",
    "                          max_words=max_words,\n",
    "                          mask=mask,\n",
    "                          contour_width=1,\n",
    "                          contour_color='steelblue')\n",
    "    \n",
    "    # Generate the word cloud\n",
    "    wordcloud.generate(text)\n",
    "    \n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(figures_dir, f\"{filename}.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def extract_key_phrases(text, n=10):\n",
    "    \"\"\"\n",
    "    Extract key phrases from text using TF-IDF.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: text to analyze\n",
    "    - n: number of key phrases to extract\n",
    "    \n",
    "    Returns:\n",
    "    - key_phrases: list of key phrases\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text) < 10:\n",
    "        return []\n",
    "    \n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Create a TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3), max_features=100)\n",
    "    \n",
    "    # Fit the vectorizer to the sentences\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Calculate the average TF-IDF score for each feature\n",
    "        avg_tfidf = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
    "        \n",
    "        # Get the indices of the top n features\n",
    "        top_indices = avg_tfidf.argsort()[-n:][::-1]\n",
    "        \n",
    "        # Get the top n features\n",
    "        key_phrases = [feature_names[i] for i in top_indices]\n",
    "        \n",
    "        return key_phrases\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting key phrases: {e}\")\n",
    "        return []\n",
    "\n",
    "def calculate_correlation(df, x_col, y_col, method='pearson'):\n",
    "    \"\"\"\n",
    "    Calculate correlation between two columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - x_col: name of the first column\n",
    "    - y_col: name of the second column\n",
    "    - method: correlation method ('pearson' or 'spearman')\n",
    "    \n",
    "    Returns:\n",
    "    - corr: correlation coefficient\n",
    "    - p_value: p-value\n",
    "    \"\"\"\n",
    "    if method == 'pearson':\n",
    "        corr, p_value = pearsonr(df[x_col], df[y_col])\n",
    "    elif method == 'spearman':\n",
    "        corr, p_value = spearmanr(df[x_col], df[y_col])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown correlation method: {method}\")\n",
    "    \n",
    "    return corr, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Preprocessed Data\n",
    "\n",
    "Let's load the preprocessed data from Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the analyzed expanded dataset\n",
    "analyzed_expanded_path = os.path.join(expanded_dir, 'analyzed_expanded_dataset.csv')\n",
    "if os.path.exists(analyzed_expanded_path):\n",
    "    analyzed_df = pd.read_csv(analyzed_expanded_path)\n",
    "    print(f\"Loaded analyzed expanded dataset: {analyzed_df.shape}\")\n",
    "else:\n",
    "    print(\"Analyzed expanded dataset not found. Please run Part 2 first.\")\n",
    "    analyzed_df = None\n",
    "\n",
    "# Load the medical terminology data\n",
    "medical_terminology_path = os.path.join(expanded_dir, 'medical_terminology.csv')\n",
    "if os.path.exists(medical_terminology_path):\n",
    "    medical_terminology_df = pd.read_csv(medical_terminology_path)\n",
    "    print(f\"Loaded medical terminology data: {medical_terminology_df.shape}\")\n",
    "else:\n",
    "    print(\"Medical terminology data not found. Please run Part 2 first.\")\n",
    "    medical_terminology_df = None\n",
    "\n",
    "# Load the clinical trials data\n",
    "clinical_trials_path = os.path.join(expanded_dir, 'clinical_trials.csv')\n",
    "if os.path.exists(clinical_trials_path):\n",
    "    clinical_trials_df = pd.read_csv(clinical_trials_path)\n",
    "    print(f\"Loaded clinical trials data: {clinical_trials_df.shape}\")\n",
    "else:\n",
    "    print(\"Clinical trials data not found. Please run Part 2 first.\")\n",
    "    clinical_trials_df = None\n",
    "\n",
    "# Load the PubMed data\n",
    "pubmed_path = os.path.join(expanded_dir, 'pubmed.csv')\n",
    "if os.path.exists(pubmed_path):\n",
    "    pubmed_df = pd.read_csv(pubmed_path)\n",
    "    print(f\"Loaded PubMed data: {pubmed_df.shape}\")\n",
    "else:\n",
    "    print(\"PubMed data not found. Please run Part 2 first.\")\n",
    "    pubmed_df = None\n",
    "\n",
    "# Load the preprocessing summary\n",
    "preprocessing_summary_path = os.path.join(expanded_dir, 'preprocessing_summary.json')\n",
    "if os.path.exists(preprocessing_summary_path):\n",
    "    with open(preprocessing_summary_path, 'r') as f:\n",
    "        preprocessing_summary = json.load(f)\n",
    "    print(\"Loaded preprocessing summary\")\n",
    "else:\n",
    "    print(\"Preprocessing summary not found. Please run Part 2 first.\")\n",
    "    preprocessing_summary = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the loaded data\n",
    "if analyzed_df is not None:\n",
    "    verify_dataframe(analyzed_df, \"Analyzed Expanded Dataset\")\n",
    "    \n",
    "if medical_terminology_df is not None:\n",
    "    verify_dataframe(medical_terminology_df, \"Medical Terminology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Analysis of Questions\n",
    "\n",
    "Let's perform a detailed text analysis of the dismissed questions and better questions to understand the linguistic differences between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we already have the text analysis data\n",
    "text_analysis_df = load_checkpoint(\"text_analysis\")\n",
    "\n",
    "# If not, perform text analysis\n",
    "if text_analysis_df is None and analyzed_df is not None:\n",
    "    # Create a copy of the analyzed dataframe\n",
    "    text_analysis_df = analyzed_df.copy()\n",
    "    \n",
    "    # Calculate the number of sentences in each question\n",
    "    text_analysis_df['DismissedQuestion_Sentences'] = text_analysis_df['DismissedQuestion'].apply(\n",
    "        lambda x: len(sent_tokenize(x)) if isinstance(x, str) else 0\n",
    "    )\n",
    "    text_analysis_df['BetterQuestion_Sentences'] = text_analysis_df['BetterQuestion'].apply(\n",
    "        lambda x: len(sent_tokenize(x)) if isinstance(x, str) else 0\n",
    "    )\n",
    "    \n",
    "    # Calculate the number of words in each question\n",
    "    text_analysis_df['DismissedQuestion_Words'] = text_analysis_df['DismissedQuestion'].apply(\n",
    "        lambda x: len(word_tokenize(x)) if isinstance(x, str) else 0\n",
    "    )\n",
    "    text_analysis_df['BetterQuestion_Words'] = text_analysis_df['BetterQuestion'].apply(\n",
    "        lambda x: len(word_tokenize(x)) if isinstance(x, str) else 0\n",
    "    )\n",
    "    \n",
    "    # Calculate the average word length in each question\n",
    "    def avg_word_length(text):\n",
    "        if not isinstance(text, str) or len(text) == 0:\n",
    "            return 0\n",
    "        words = word_tokenize(text)\n",
    "        if len(words) == 0:\n",
    "            return 0\n",
    "        return sum(len(word) for word in words) / len(words)\n",
    "    \n",
    "    text_analysis_df['DismissedQuestion_AvgWordLength'] = text_analysis_df['DismissedQuestion'].apply(avg_word_length)\n",
    "    text_analysis_df['BetterQuestion_AvgWordLength'] = text_analysis_df['BetterQuestion'].apply(avg_word_length)\n",
    "    \n",
    "    # Extract key phrases from each question\n",
    "    text_analysis_df['DismissedQuestion_KeyPhrases'] = text_analysis_df['DismissedQuestion'].apply(\n",
    "        lambda x: '; '.join(extract_key_phrases(x, n=5))\n",
    "    )\n",
    "    text_analysis_df['BetterQuestion_KeyPhrases'] = text_analysis_df['BetterQuestion'].apply(\n",
    "        lambda x: '; '.join(extract_key_phrases(x, n=5))\n",
    "    )\n",
    "    \n",
    "    # Calculate the specificity score (ratio of unique words to total words)\n",
    "    def calculate_specificity(text):\n",
    "        if not isinstance(text, str) or len(text) == 0:\n",
    "            return 0\n",
    "        words = word_tokenize(text.lower())\n",
    "        if len(words) == 0:\n",
    "            return 0\n",
    "        unique_words = set(words)\n",
    "        return len(unique_words) / len(words)\n",
    "    \n",
    "    text_analysis_df['DismissedQuestion_Specificity'] = text_analysis_df['DismissedQuestion'].apply(calculate_specificity)\n",
    "    text_analysis_df['BetterQuestion_Specificity'] = text_analysis_df['BetterQuestion'].apply(calculate_specificity)\n",
    "    \n",
    "    # Calculate the question complexity (product of number of sentences and average word length)\n",
    "    text_analysis_df['DismissedQuestion_Complexity'] = text_analysis_df['DismissedQuestion_Sentences'] * text_analysis_df['DismissedQuestion_AvgWordLength']\n",
    "    text_analysis_df['BetterQuestion_Complexity'] = text_analysis_df['BetterQuestion_Sentences'] * text_analysis_df['BetterQuestion_AvgWordLength']\n",
    "    \n",
    "    # Calculate the complexity ratio\n",
    "    text_analysis_df['Complexity_Ratio'] = text_analysis_df['BetterQuestion_Complexity'] / text_analysis_df['DismissedQuestion_Complexity'].replace(0, 0.1)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    save_checkpoint(text_analysis_df, \"text_analysis\")\n",
    "else:\n",
    "    print(\"Using existing text analysis data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the text analysis data\n",
    "if text_analysis_df is not None:\n",
    "    verify_dataframe(text_analysis_df, \"Text Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sentence count comparison\n",
    "if text_analysis_df is not None:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create a bar chart\n",
    "    avg_dismissed_sentences = text_analysis_df['DismissedQuestion_Sentences'].mean()\n",
    "    avg_better_sentences = text_analysis_df['BetterQuestion_Sentences'].mean()\n",
    "    avg_ratio = avg_better_sentences / avg_dismissed_sentences\n",
    "    \n",
    "    bars = plt.bar(['Dismissed Question', 'Better Question'], \n",
    "                   [avg_dismissed_sentences, avg_better_sentences],\n",
    "                   color=['#ff7f0e', '#2ca02c'])\n",
    "    \n",
    "    plt.title('Average Number of Sentences Comparison', fontsize=16)\n",
    "    plt.ylabel('Average Number of Sentences', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add data labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                 f'{height:.1f}',\n",
    "                 ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    # Add ratio annotation\n",
    "    plt.annotate(f'Better questions have on average {avg_ratio:.1f}x more sentences than dismissed questions',\n",
    "                xy=(0.5, 0.9), xycoords='axes fraction',\n",
    "                ha='center', va='center',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8),\n",
    "                fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'sentence_count_comparison.png'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the word count comparison\n",
    "if text_analysis_df is not None:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create a bar chart\n",
    "    avg_dismissed_words = text_analysis_df['DismissedQuestion_Words'].mean()\n",
    "    avg_better_words = text_analysis_df['BetterQuestion_Words'].mean()\n",
    "    avg_ratio = avg_better_words / avg_dismissed_words\n",
    "    \n",
    "    bars = plt.bar(['Dismissed Question', 'Better Question'], \n",
    "                   [avg_dismissed_words, avg_better_words],\n",
    "                   color=['#ff7f0e', '#2ca02c'])\n",
    "    \n",
    "    plt.title('Average Number of Words Comparison', fontsize=16)\n",
    "    plt.ylabel('Average Number of Words', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add data labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                 f'{height:.1f}',\n",
    "                 ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    # Add ratio annotation\n",
    "    plt.annotate(f'Better questions have on average {avg_ratio:.1f}x more words than dismissed questions',\n",
    "                xy=(0.5, 0.9), xycoords='axes fraction',\n",
    "                ha='center', va='center',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8),\n",
    "                fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'word_count_comparison.png'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the specificity comparison\n",
    "if text_analysis_df is not None:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create a bar chart\n",
    "    avg_dismissed_specificity = text_analysis_df['DismissedQuestion_Specificity'].mean()\n",
    "    avg_better_specificity = text_analysis_df['BetterQuestion_Specificity'].mean()\n",
    "    avg_ratio = avg_better_specificity / avg_dismissed_specificity\n",
    "    \n",
    "    bars = plt.bar(['Dismissed Question', 'Better Question'], \n",
    "                   [avg_dismissed_specificity, avg_better_specificity],\n",
    "                   color=['#ff7f0e', '#2ca02c'])\n",
    "    \n",
    "    plt.title('Average Specificity Comparison', fontsize=16)\n",
    "    plt.ylabel('Average Specificity Score', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add data labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{height:.2f}',\n",
    "                 ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    # Add ratio annotation\n",
    "    plt.annotate(f'Better questions are {avg_ratio:.2f}x more specific than dismissed questions',\n",
    "                xy=(0.5, 0.9), xycoords='axes fraction',\n",
    "                ha='center', va='center',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8),\n",
    "                fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'specificity_comparison.png'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for dismissed and better questions\n",
    "if text_analysis_df is not None:\n",
    "    # Combine all dismissed questions\n",
    "    all_dismissed = ' '.join(text_analysis_df['DismissedQuestion'].dropna())\n",
    "    \n",
    "    # Combine all better questions\n",
    "    all_better = ' '.join(text_analysis_df['BetterQuestion'].dropna())\n",
    "    \n",
    "    # Create word clouds\n",
    "    create_wordcloud(all_dismissed, 'Word Cloud of Dismissed Questions', 'dismissed_questions_wordcloud', \n",
    "                     colormap='Oranges')\n",
    "    create_wordcloud(all_better, 'Word Cloud of Better Questions', 'better_questions_wordcloud', \n",
    "                     colormap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the key phrases\n",
    "if text_analysis_df is not None:\n",
    "    # Extract all key phrases\n",
    "    dismissed_key_phrases = []\n",
    "    for phrases in text_analysis_df['DismissedQuestion_KeyPhrases'].dropna():\n",
    "        dismissed_key_phrases.extend([phrase.strip() for phrase in phrases.split(';') if phrase.strip()])\n",
    "    \n",
    "    better_key_phrases = []\n",
    "    for phrases in text_analysis_df['BetterQuestion_KeyPhrases'].dropna():\n",
    "        better_key_phrases.extend([phrase.strip() for phrase in phrases.split(';') if phrase.strip()])\n",
    "    \n",
    "    # Count the frequency of each key phrase\n",
    "    dismissed_phrase_counts = pd.Series(dismissed_key_phrases).value_counts().head(15)\n",
    "    better_phrase_counts = pd.Series(better_key_phrases).value_counts().head(15)\n",
    "    \n",
    "    # Plot the top key phrases for dismissed questions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    dismissed_phrase_counts.plot(kind='barh', color='#ff7f0e')\n",
    "    plt.title('Top Key Phrases in Dismissed Questions', fontsize=16)\n",
    "    plt.xlabel('Frequency', fontsize=12)\n",
    "    plt.ylabel('Key Phrase', fontsize=12)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'dismissed_key_phrases.png'), dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the top key phrases for better questions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    better_phrase_counts.plot(kind='barh', color='#2ca02c')\n",
    "    plt.title('Top Key Phrases in Better Questions', fontsize=16)\n",
    "    plt.xlabel('Frequency', fontsize=12)\n",
    "    plt.ylabel('Key Phrase', fontsize=12)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'better_key_phrases.png'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis\n",
    "\n",
    "Let's analyze the correlations between different question characteristics and dismissal frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we already have the correlation analysis data\n",
    "correlation_df = load_checkpoint(\"correlation_analysis\")\n",
    "\n",
    "# If not, perform correlation analysis\n",
    "if correlation_df is None and text_analysis_df is not None:\n",
    "    # Create a dataframe to store correlation results\n",
    "    correlation_df = pd.DataFrame(columns=['Variable1', 'Variable2', 'Correlation', 'P_Value', 'Method'])\n",
    "    \n",
    "    # Convert dismissal frequency to numeric\n",
    "    dismissal_map = {'Low': 1, 'Medium': 2, 'High': 3, 'Very High': 4}\n",
    "    text_analysis_df['DismissalFrequency_Numeric'] = text_analysis_df['DismissalFrequency'].map(dismissal_map)\n",
    "    \n",
    "    # Define the variables to analyze\n",
    "    question_vars = [\n",
    "        'DismissedQuestion_Length', 'BetterQuestion_Length', 'Question_Length_Ratio',\n",
    "        'DismissedQuestion_Sentences', 'BetterQuestion_Sentences',\n",
    "        'DismissedQuestion_Words', 'BetterQuestion_Words',\n",
    "        'DismissedQuestion_AvgWordLength', 'BetterQuestion_AvgWordLength',\n",
    "        'DismissedQuestion_Specificity', 'BetterQuestion_Specificity',\n",
    "        'DismissedQuestion_Complexity', 'BetterQuestion_Complexity', 'Complexity_Ratio'\n",
    "    ]\n",
    "    \n",
    "    # Calculate correlations with dismissal frequency\n",
    "    for var in question_vars:\n",
    "        if var in text_analysis_df.columns:\n",
    "            # Pearson correlation\n",
    "            pearson_corr, pearson_p = calculate_correlation(text_analysis_df, var, 'DismissalFrequency_Numeric', method='pearson')\n",
    "            correlation_df = pd.concat([correlation_df, pd.DataFrame({\n",
    "                'Variable1': [var],\n",
    "                'Variable2': ['DismissalFrequency'],\n",
    "                'Correlation': [pearson_corr],\n",
    "                'P_Value': [pearson_p],\n",
    "                'Method': ['Pearson']\n",
    "            })], ignore_index=True)\n",
    "            \n",
    "            # Spearman correlation\n",
    "            spearman_corr, spearman_p = calculate_correlation(text_analysis_df, var, 'DismissalFrequency_Numeric', method='spearman')\n",
    "            correlation_df = pd.concat([correlation_df, pd.DataFrame({\n",
    "                'Variable1': [var],\n",
    "                'Variable2': ['DismissalFrequency'],\n",
    "                'Correlation': [spearman_corr],\n",
    "                'P_Value': [spearman_p],\n",
    "                'Method': ['Spearman']\n",
    "            })], ignore_index=True)\n",
    "    \n",
    "    # Calculate correlations with diagnosis delay\n",
    "    for var in question_vars:\n",
    "        if var in text_analysis_df.columns:\n",
    "            # Pearson correlation\n",
    "            pearson_corr, pearson_p = calculate_correlation(text_analysis_df, var, 'DiagnosisDelay', method='pearson')\n",
    "            correlation_df = pd.concat([correlation_df, pd.DataFrame({\n",
    "                'Variable1': [var],\n",
    "                'Variable2': ['DiagnosisDelay'],\n",
    "                'Correlation': [pearson_corr],\n",
    "                'P_Value': [pearson_p],\n",
    "                'Method': ['Pearson']\n",
    "            })], ignore_index=True)\n",
    "            \n",
    "            # Spearman correlation\n",
    "            spearman_corr, spearman_p = calculate_correlation(text_analysis_df, var, 'DiagnosisDelay', method='spearman')\n",
    "            correlation_df = pd.concat([correlation_df, pd.DataFrame({\n",
    "                'Variable1': [var],\n",
    "                'Variable2': ['DiagnosisDelay'],\n",
    "                'Correlation': [spearman_corr],\n",
    "                'P_Value': [spearman_p],\n",
    "                'Method': ['Spearman']\n",
    "            })], ignore_index=True)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    save_checkpoint(correlation_df, \"correlation_analysis\")\n",
    "else:\n",
    "    print(\"Using existing correlation analysis data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the correlation analysis data\n",
    "if correlation_df is not None:\n",
    "    verify_dataframe(correlation_df, \"Correlation Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the top correlations with dismissal frequency\n",
    "if correlation_df is not None:\n",
    "    # Filter for Spearman correlations with dismissal frequency\n",
    "    dismissal_corrs = correlation_df[\n",
    "        (correlation_df['Variable2'] == 'DismissalFrequency') & \n",
    "        (correlation_df['Method'] == 'Spearman')\n",
    "    ].copy()\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    dismissal_corrs['Abs_Correlation'] = dismissal_corrs['Correlation'].abs()\n",
    "    dismissal_corrs = dismissal_corrs.sort_values('Abs_Correlation', ascending=False).head(10)\n",
    "    \n",
    "    # Plot the correlations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.barh(dismissal_corrs['Variable1'], dismissal_corrs['Correlation'], \n",
    "                   color=dismissal_corrs['Correlation'].apply(lambda x: '#2ca02c' if x > 0 else '#d62728'))\n",
    "    \n",
    "    plt.title('Top Correlations with Dismissal Frequency', fontsize=16)\n",
    "    plt.xlabel('Spearman Correlation Coefficient', fontsize=12)\n",
    "    plt.ylabel('Question Characteristic', fontsize=12)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add data labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        x_pos = width + 0.03 if width > 0 else width - 0.15\n",
    "        plt.text(x_pos, bar.get_y() + bar.get_height()/2,\n",
    "                 f'{width:.2f}',\n",
    "                 va='center', fontsize=10,\n",
    "                 color='black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'dismissal_frequency_correlations.png'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the top correlations with diagnosis delay\n",
    "if correlation_df is not None:\n",
    "    # Filter for Spearman correlations with diagnosis delay\n",
    "    delay_corrs = correlation_df[\n",
    "        (correlation_df['Variable2'] == 'DiagnosisDelay') & \n",
    "        (correlation_df['Method'] == 'Spearman')\n",
    "    ].copy()\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    delay_corrs['Abs_Correlation'] = delay_corrs['Correlation'].abs()\n",
    "    delay_corrs = delay_corrs.sort_values('Abs_Correlation', ascending=False).head(10)\n",
    "    \n",
    "    # Plot the correlations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.barh(delay_corrs['Variable1'], delay_corrs['Correlation'], \n",
    "                   color=delay_corrs['Correlation'].apply(lambda x: '#2ca02c' if x > 0 else '#d62728'))\n",
    "    \n",
    "    plt.title('Top Correlations with Diagnosis Delay', fontsize=16)\n",
    "    plt.xlabel('Spearman Correlation Coefficient', fontsize=12)\n",
    "    plt.ylabel('Question Characteristic', fontsize=12)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add data labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        x_pos = width + 0.03 if width > 0 else width - 0.15\n",
    "        plt.text(x_pos, bar.get_y() + bar.get_height()/2,\n",
    "                 f'{width:.2f}',\n",
    "                 va='center', fontsize=10,\n",
    "                 color='black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'diagnosis_delay_correlations.png'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demographic Analysis\n",
    "\n",
    "Let's analyze how dismissal patterns vary across different demographic groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we already have the demographic analysis data\n",
    "demographic_analysis_df = load_checkpoint(\"demographic_analysis\")\n",
    "\n",
    "# If not, perform demographic analysis\n",
    "if demographic_analysis_df is None and text_analysis_df is not None:\n",
    "    # Create a copy of the text analysis dataframe\n",
    "    demographic_analysis_df = text_analysis_df.copy()\n",
    "    \n",
    "    # Convert dismissal frequency to numeric\n",
    "    if 'DismissalFrequency_Numeric' not in demographic_analysis_df.columns:\n",
    "        dismissal_map = {'Low': 1, 'Medium': 2, 'High': 3, 'Very High': 4}\n",
    "        demographic_analysis_df['DismissalFrequency_Numeric'] = demographic_analysis_df['DismissalFrequency'].map(dismissal_map)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    save_checkpoint(demographic_analysis_df, \"demographic_analysis\")\n",
    "else:\n",
    "    print(\"Using existing demographic analysis data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the demographic analysis data\n",
    "if demographic_analysis_df is not None:\n",
    "    verify_dataframe(demographic_analysis_df, \"Demographic Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dismissal frequency by age group\n",
    "if demographic_analysis_df is not None:\n",
    "    # Calculate the average dismissal frequency by age group\n",
    "    age_dismissal = demographic_analysis_df.groupby('AgeGroup')['DismissalFrequency_Numeric'].mean().reset_index()\n",
    "    \n",
    "    # Define the order for age groups\n",
    "    order = ['18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "    age_dismissal['AgeGroup'] = pd.Categorical(age_dismissal['AgeGroup'], categories=order, ordered=True)\n",
    "    age_dismissal = age_dismissal.sort_values('AgeGroup')\n",
    "    \n",
    "    # Create a color map based on dismissal frequency\n",
    "    norm = Normalize(vmin=age_dismissal['DismissalFrequency_Numeric'].min(), \n",
    "                     vmax=age_dismissal['DismissalFrequency_Numeric'].max())\n",
    "    colors = plt.cm.Reds(norm(age_dismissal['DismissalFrequency_Numeric']))\n",
    "    \n",
    "    # Plot the dismissal frequency by age group\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(age_dismissal['AgeGroup'], age_dismissal['DismissalFrequency_Numeric'], color=colors)\n",
    "    \n",
    "    plt.title('Average Dismissal Frequency by Age Group', fontsize=16)\n",
    "    plt.xlabel('Age Group', fontsize=12)\n",
    "    plt.ylabel('Average Dismissal Frequency (1=Low, 4=Very High)', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add data labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                 f'{height:.2f}',\n",
    "                 ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'dismissal_by_age_group.png'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dismissal frequency by racial/ethnic group\n",
    "if demographic_analysis_df is not None:\n",
    "    # Calculate the average dismissal frequency by racial/ethnic group\n",
    "    racial_dismissal = demographic_analysis_df.groupby('RacialEthnicConsiderations')['DismissalFrequency_Numeric'].mean().reset_index()\n",
    "    \n",
    "    # Sort by dismissal frequency\n",
    "    racial_dismissal = racial_dismissal.sort_values('DismissalFrequency_Numeric', ascending=False)\n",
    "    \n",
    "    # Create a color map based on dismissal frequency\n",
    "    norm = Normalize(vmin=racial_dismissal['DismissalFrequency_Numeric'].min(), \n",
    "                     vmax=racial_dismissal['DismissalFrequency_Numeric'].max())\n",
    "    colors = plt.cm.Reds(norm(racial_dismissal['DismissalFrequency_Numeric']))\n",
    "    \n",
    "    # Plot the dismissal frequency by racial/ethnic group\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(racial_dismissal['RacialEthnicConsiderations'], racial_dismissal['DismissalFrequency_Numeric'], color=colors)\n",
    "    \n",
    "    plt.title('Average Dismissal Frequency by Racial/Ethnic Group', fontsize=16)\n",
    "    plt.xlabel('Racial/Ethnic Group', fontsize=12)\n",
    "    plt.ylabel('Average Dismissal Frequency (1=Low, 4=Very High)', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add data labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                 f'{height:.2f}',\n",
    "                 ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'dismissal_by_racial_ethnic_group.png'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize diagnosis delay by age group\n",
    "if demographic_analysis_df is not None:\n",
    "    # Calculate the average diagnosis delay by age group\n",
    "    age_delay = demographic_analysis_df.groupby('AgeGroup')['DiagnosisDelay'].mean().reset_index()\n",
    "    \n",
    "    # Define the order for age groups\n",
    "    order = ['18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "    age_delay['AgeGroup'] = pd.Categorical(age_delay['AgeGroup'], categories=order, ordered=True)\n",
    "    age_delay = age_delay.sort_values('AgeGroup')\n",
    "    \n",
    "    # Create a color map based on diagnosis delay\n",
    "    norm = Normalize(vmin=age_delay['DiagnosisDelay'].min(), \n",
    "                     vmax=age_delay['DiagnosisDelay'].max())\n",
    "    colors = plt.cm.Reds(norm(age_delay['DiagnosisDelay']))\n",
    "    \n",
    "    # Plot the diagnosis delay by age group\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(age_delay['AgeGroup'], age_delay['DiagnosisDelay'], color=colors)\n",
    "    \n",
    "    plt.title('Average Diagnosis Delay by Age Group', fontsize=16)\n",
    "    plt.xlabel('Age Group', fontsize=12)\n",
    "    plt.ylabel('Average Diagnosis Delay (years)', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add data labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                 f'{height:.1f}',\n",
    "                 ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'delay_by_age_group.png'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize diagnosis delay by racial/ethnic group\n",
    "if demographic_analysis_df is not None:\n",
    "    # Calculate the average diagnosis delay by racial/ethnic group\n",
    "    racial_delay = demographic_analysis_df.groupby('RacialEthnicConsiderations')['DiagnosisDelay'].mean().reset_index()\n",
    "    \n",
    "    # Sort by diagnosis delay\n",
    "    racial_delay = racial_delay.sort_values('DiagnosisDelay', ascending=False)\n",
    "    \n",
    "    # Create a color map based on diagnosis delay\n",
    "    norm = Normalize(vmin=racial_delay['DiagnosisDelay'].min(), \n",
    "                     vmax=racial_delay['DiagnosisDelay'].max())\n",
    "    colors = plt.cm.Reds(norm(racial_delay['DiagnosisDelay']))\n",
    "    \n",
    "    # Plot the diagnosis delay by racial/ethnic group\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(racial_delay['RacialEthnicConsiderations'], racial_delay['DiagnosisDelay'], color=colors)\n",
    "    \n",
    "    plt.title('Average Diagnosis Delay by Racial/Ethnic Group', fontsize=16)\n",
    "    plt.xlabel('Racial/Ethnic Group', fontsize=12)\n",
    "    plt.ylabel('Average Diagnosis Delay (years)', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add data labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                 f'{height:.1f}',\n",
    "                 ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'delay_by_racial_ethnic_group.png'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Question Transformation Analysis\n",
    "\n",
    "Let's analyze how dismissed questions transform into better questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we already have the transformation analysis data\n",
    "transformation_df = load_checkpoint(\"transformation_analysis\")\n",
    "\n",
    "# If not, perform transformation analysis\n",
    "if transformation_df is None and demographic_analysis_df is not None:\n",
    "    # Create a copy of the demographic analysis dataframe\n",
    "    transformation_df = demographic_analysis_df.copy()\n",
    "    \n",
    "    # Calculate the transformation metrics\n",
    "    transformation_df['Length_Increase'] = transformation_df['BetterQuestion_Length'] - transformation_df['DismissedQuestion_Length']\n",
    "    transformation_df['Word_Increase'] = transformation_df['BetterQuestion_Words'] - transformation_df['DismissedQuestion_Words']\n",
    "    transformation_df['Sentence_Increase'] = transformation_df['BetterQuestion_Sentences'] - transformation_df['DismissedQuestion_Sentences']\n",
    "    transformation_df['Specificity_Increase'] = transformation_df['BetterQuestion_Specificity'] - transformation_df['DismissedQuestion_Specificity']\n",
    "    transformation_df['Complexity_Increase'] = transformation_df['BetterQuestion_Complexity'] - transformation_df['DismissedQuestion_Complexity']\n",
    "    \n",
    "    # Calculate the percentage increases\n",
    "    transformation_df['Length_Increase_Pct'] = (transformation_df['Length_Increase'] / transformation_df['DismissedQuestion_Length']) * 100\n",
    "    transformation_df['Word_Increase_Pct'] = (transformation_df['Word_Increase'] / transformation_df['DismissedQuestion_Words']) * 100\n",
    "    transformation_df['Sentence_Increase_Pct'] = (transformation_df['Sentence_Increase'] / transformation_df['DismissedQuestion_Sentences'].replace(0, 1)) * 100\n",
    "    transformation_df['Specificity_Increase_Pct'] = (transformation_df['Specificity_Increase'] / transformation_df['DismissedQuestion_Specificity']) * 100\n",
    "    transformation_df['Complexity_Increase_Pct'] = (transformation_df['Complexity_Increase'] / transformation_df['DismissedQuestion_Complexity'].replace(0, 0.1)) * 100\n",
    "    \n",
    "    # Save checkpoint\n",
    "    save_checkpoint(transformation_df, \"transformation_analysis\")\n",
    "else:\n",
    "    print(\"Using existing transformation analysis data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the transformation analysis data\n",
    "if transformation_df is not None:\n",
    "    verify_dataframe(transformation_df, \"Transformation Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the average transformation metrics\n",
    "if transformation_df is not None:\n",
    "    # Calculate the average transformation metrics\n",
    "    avg_metrics = {\n",
    "        'Length Increase': transformation_df['Length_Increase'].mean(),\n",
    "        'Word Increase': transformation_df['Word_Increase'].mean(),\n",
    "        'Sentence Increase': transformation_df['Sentence_Increase'].mean(),\n",
    "        'Specificity Increase': transformation_df['Specificity_Increase'].mean(),\n",
    "        'Complexity Increase': transformation_df['Complexity_Increase'].mean()\n",
    "    }\n",
    "    \n",
    "    # Plot the average transformation metrics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(avg_metrics.keys(), avg_metrics.values(), color=sns.color_palette(\"viridis\", len(avg_metrics)))\n",
    "    \n",
    "    plt.title('Average Question Transformation Metrics', fontsize=16)\n",
    "    plt.ylabel('Average Increase', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add data labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                 f'{height:.1f}',\n",
    "                 ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'transformation_metrics.png'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the average percentage increases\n",
    "if transformation_df is not None:\n",
    "    # Calculate the average percentage increases\n",
    "    avg_pct_increases = {\n",
    "        'Length': transformation_df['Length_Increase_Pct'].mean(),\n",
    "        'Words': transformation_df['Word_Increase_Pct'].mean(),\n",
    "        'Sentences': transformation_df['Sentence_Increase_Pct'].mean(),\n",
    "        'Specificity': transformation_df['Specificity_Increase_Pct'].mean(),\n",
    "        'Complexity': transformation_df['Complexity_Increase_Pct'].mean()\n",
    "    }\n",
    "    \n",
    "    # Plot the average percentage increases\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(avg_pct_increases.keys(), avg_pct_increases.values(), color=sns.color_palette(\"viridis\", len(avg_pct_increases)))\n",
    "    \n",
    "    plt.title('Average Percentage Increases in Question Transformation', fontsize=16)\n",
    "    plt.ylabel('Average Percentage Increase (%)', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add data labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 10,\n",
    "                 f'{height:.0f}%',\n",
    "                 ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'transformation_percentage_increases.png'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the transformation metrics by category\n",
    "if transformation_df is not None:\n",
    "    # Calculate the average word increase by category\n",
    "    category_word_increase = transformation_df.groupby('Category')['Word_Increase'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    # Plot the average word increase by category\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(category_word_increase.index, category_word_increase.values, \n",
    "                   color=sns.color_palette(\"viridis\", len(category_word_increase)))\n",
    "    \n",
    "    plt.title('Average Word Increase by Category', fontsize=16)\n",
    "    plt.xlabel('Category', fontsize=12)\n",
    "    plt.ylabel('Average Word Increase', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add data labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                 f'{height:.1f}',\n",
    "                 ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'word_increase_by_category.png'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Identify Key Factors for Question Effectiveness\n",
    "\n",
    "Based on our analysis, let's identify the key factors that contribute to question effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of key factors\n",
    "key_factors = {\n",
    "    \"question_length\": {\n",
    "        \"finding\": \"Better questions are significantly longer than dismissed questions\",\n",
    "        \"evidence\": f\"Average length ratio: {preprocessing_summary['question_analysis']['avg_length_ratio']:.1f}x\",\n",
    "        \"recommendation\": \"Encourage patients to provide more detailed questions with context\"\n",
    "    },\n",
    "    \"medical_terminology\": {\n",
    "        \"finding\": \"Better questions contain more medical terminology\",\n",
    "        \"evidence\": f\"Average medical terms difference: {preprocessing_summary['question_analysis']['avg_medical_terms_difference']:.1f}\",\n",
    "        \"recommendation\": \"Include relevant medical terms in questions to improve specificity\"\n",
    "    },\n",
    "    \"question_specificity\": {\n",
    "        \"finding\": \"Better questions are more specific\",\n",
    "        \"evidence\": f\"Average specificity increase: {transformation_df['Specificity_Increase'].mean():.2f}\",\n",
    "        \"recommendation\": \"Include specific details about symptoms, duration, and context\"\n",
    "    },\n",
    "    \"sentence_structure\": {\n",
    "        \"finding\": \"Better questions contain more sentences\",\n",
    "        \"evidence\": f\"Average sentence increase: {transformation_df['Sentence_Increase'].mean():.1f}\",\n",
    "        \"recommendation\": \"Structure questions with multiple sentences covering different aspects\"\n",
    "    },\n",
    "    \"question_complexity\": {\n",
    "        \"finding\": \"Better questions are more complex\",\n",
    "        \"evidence\": f\"Average complexity increase: {transformation_df['Complexity_Increase'].mean():.1f}\",\n",
    "        \"recommendation\": \"Include both simple and complex sentences to convey information effectively\"\n",
    "    },\n",
    "    \"demographic_context\": {\n",
    "        \"finding\": \"Dismissal patterns vary by demographic group\",\n",
    "        \"evidence\": \"Highest dismissal rates observed in specific age and racial/ethnic groups\",\n",
    "        \"recommendation\": \"Tailor questions based on demographic context and known dismissal patterns\"\n",
    "    },\n",
    "    \"symptom_details\": {\n",
    "        \"finding\": \"Better questions include detailed symptom descriptions\",\n",
    "        \"evidence\": \"Key phrases in better questions focus on symptom characteristics\",\n",
    "        \"recommendation\": \"Describe symptoms in detail, including onset, duration, severity, and triggers\"\n",
    "    },\n",
    "    \"medical_history\": {\n",
    "        \"finding\": \"Better questions reference relevant medical history\",\n",
    "        \"evidence\": \"Better questions often mention family history and previous conditions\",\n",
    "        \"recommendation\": \"Include relevant personal and family medical history in questions\"\n",
    "    },\n",
    "    \"question_structure\": {\n",
    "        \"finding\": \"Better questions have a clear structure\",\n",
    "        \"evidence\": \"Better questions typically start with context, then details, then specific query\",\n",
    "        \"recommendation\": \"Structure questions with context first, then details, then specific query\"\n",
    "    },\n",
    "    \"impact_description\": {\n",
    "        \"finding\": \"Better questions describe impact on daily life\",\n",
    "        \"evidence\": \"Better questions often mention how symptoms affect daily activities\",\n",
    "        \"recommendation\": \"Describe how symptoms or conditions impact daily life and functioning\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the key factors as JSON\n",
    "with open(os.path.join(analysis_dir, 'key_factors.json'), 'w') as f:\n",
    "    json.dump(key_factors, f, indent=2)\n",
    "\n",
    "print(\"Key factors saved to:\", os.path.join(analysis_dir, 'key_factors.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the key factors\n",
    "print(\"\\n--- Key Factors for Question Effectiveness ---\")\n",
    "for factor, details in key_factors.items():\n",
    "    print(f\"\\n{factor.replace('_', ' ').title()}:\")\n",
    "    print(f\"  Finding: {details['finding']}\")\n",
    "    print(f\"  Evidence: {details['evidence']}\")\n",
    "    print(f\"  Recommendation: {details['recommendation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prepare Data for Next Notebook\n",
    "\n",
    "Let's save the analyzed data for use in the next notebook in the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformation analysis data to the analysis directory\n",
    "if transformation_df is not None:\n",
    "    transformation_df.to_csv(os.path.join(analysis_dir, 'transformation_analysis.csv'), index=False)\n",
    "    print(f\"Saved transformation analysis data to: {os.path.join(analysis_dir, 'transformation_analysis.csv')}\")\n",
    "\n",
    "# Save the text analysis data to the analysis directory\n",
    "if text_analysis_df is not None:\n",
    "    text_analysis_df.to_csv(os.path.join(analysis_dir, 'text_analysis.csv'), index=False)\n",
    "    print(f\"Saved text analysis data to: {os.path.join(analysis_dir, 'text_analysis.csv')}\")\n",
    "\n",
    "# Save the correlation analysis data to the analysis directory\n",
    "if correlation_df is not None:\n",
    "    correlation_df.to_csv(os.path.join(analysis_dir, 'correlation_analysis.csv'), index=False)\n",
    "    print(f\"Saved correlation analysis data to: {os.path.join(analysis_dir, 'correlation_analysis.csv')}\")\n",
    "\n",
    "# Save the demographic analysis data to the analysis directory\n",
    "if demographic_analysis_df is not None:\n",
    "    demographic_analysis_df.to_csv(os.path.join(analysis_dir, 'demographic_analysis.csv'), index=False)\n",
    "    print(f\"Saved demographic analysis data to: {os.path.join(analysis_dir, 'demographic_analysis.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of the analysis results\n",
    "analysis_summary = {\n",
    "    \"text_analysis\": {\n",
    "        \"avg_dismissed_sentences\": float(text_analysis_df['DismissedQuestion_Sentences'].mean()) if text_analysis_df is not None else None,\n",
    "        \"avg_better_sentences\": float(text_analysis_df['BetterQuestion_Sentences'].mean()) if text_analysis_df is not None else None,\n",
    "        \"avg_dismissed_words\": float(text_analysis_df['DismissedQuestion_Words'].mean()) if text_analysis_df is not None else None,\n",
    "        \"avg_better_words\": float(text_analysis_df['BetterQuestion_Words'].mean()) if text_analysis_df is not None else None,\n",
    "        \"avg_dismissed_specificity\": float(text_analysis_df['DismissedQuestion_Specificity'].mean()) if text_analysis_df is not None else None,\n",
    "        \"avg_better_specificity\": float(text_analysis_df['BetterQuestion_Specificity'].mean()) if text_analysis_df is not None else None,\n",
    "        \"avg_dismissed_complexity\": float(text_analysis_df['DismissedQuestion_Complexity'].mean()) if text_analysis_df is not None else None,\n",
    "        \"avg_better_complexity\": float(text_analysis_df['BetterQuestion_Complexity'].mean()) if text_analysis_df is not None else None\n",
    "    },\n",
    "    \"transformation_analysis\": {\n",
    "        \"avg_length_increase\": float(transformation_df['Length_Increase'].mean()) if transformation_df is not None else None,\n",
    "        \"avg_word_increase\": float(transformation_df['Word_Increase'].mean()) if transformation_df is not None else None,\n",
    "        \"avg_sentence_increase\": float(transformation_df['Sentence_Increase'].mean()) if transformation_df is not None else None,\n",
    "        \"avg_specificity_increase\": float(transformation_df['Specificity_Increase'].mean()) if transformation_df is not None else None,\n",
    "        \"avg_complexity_increase\": float(transformation_df['Complexity_Increase'].mean()) if transformation_df is not None else None,\n",
    "        \"avg_length_increase_pct\": float(transformation_df['Length_Increase_Pct'].mean()) if transformation_df is not None else None,\n",
    "        \"avg_word_increase_pct\": float(transformation_df['Word_Increase_Pct'].mean()) if transformation_df is not None else None,\n",
    "        \"avg_sentence_increase_pct\": float(transformation_df['Sentence_Increase_Pct'].mean()) if transformation_df is not None else None,\n",
    "        \"avg_specificity_increase_pct\": float(transformation_df['Specificity_Increase_Pct'].mean()) if transformation_df is not None else None,\n",
    "        \"avg_complexity_increase_pct\": float(transformation_df['Complexity_Increase_Pct'].mean()) if transformation_df is not None else None\n",
    "    },\n",
    "    \"demographic_analysis\": {\n",
    "        \"age_groups\": list(demographic_analysis_df['AgeGroup'].unique()) if demographic_analysis_df is not None else [],\n",
    "        \"racial_ethnic_groups\": list(demographic_analysis_df['RacialEthnicConsiderations'].unique()) if demographic_analysis_df is not None else [],\n",
    "        \"highest_dismissal_age_group\": demographic_analysis_df.groupby('AgeGroup')['DismissalFrequency_Numeric'].mean().idxmax() if demographic_analysis_df is not None else None,\n",
    "        \"highest_dismissal_racial_ethnic_group\": demographic_analysis_df.groupby('RacialEthnicConsiderations')['DismissalFrequency_Numeric'].mean().idxmax() if demographic_analysis_df is not None else None,\n",
    "        \"highest_delay_age_group\": demographic_analysis_df.groupby('AgeGroup')['DiagnosisDelay'].mean().idxmax() if demographic_analysis_df is not None else None,\n",
    "        \"highest_delay_racial_ethnic_group\": demographic_analysis_df.groupby('RacialEthnicConsiderations')['DiagnosisDelay'].mean().idxmax() if demographic_analysis_df is not None else None\n",
    "    },\n",
    "    \"key_factors\": list(key_factors.keys())\n",
    "}\n",
    "\n",
    "# Save the analysis summary as JSON\n",
    "with open(os.path.join(analysis_dir, 'analysis_summary.json'), 'w') as f:\n",
    "    json.dump(analysis_summary, f, indent=2)\n",
    "\n",
    "print(\"Analysis summary saved to:\", os.path.join(analysis_dir, 'analysis_summary.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we've performed a comprehensive analysis and visualization of women's health questions to identify patterns that can help our LLM model generate better questions:\n",
    "\n",
    "1. **Text Analysis**: We analyzed the linguistic characteristics of dismissed and better questions, finding that better questions are longer, more specific, and more complex.\n",
    "\n",
    "2. **Correlation Analysis**: We identified correlations between question characteristics and dismissal frequency/diagnosis delay, which can help prioritize features for our LLM model.\n",
    "\n",
    "3. **Demographic Analysis**: We analyzed how dismissal patterns vary across different demographic groups, finding significant variations that our LLM model should account for.\n",
    "\n",
    "4. **Question Transformation Analysis**: We analyzed how dismissed questions transform into better questions, identifying key patterns that our LLM model can learn from.\n",
    "\n",
    "5. **Key Factors**: We identified 10 key factors that contribute to question effectiveness, which will guide our LLM model in generating better questions.\n",
    "\n",
    "These insights will be used in the next notebook (Part 4: Training Split Preparation) to prepare the data for LLM training, ensuring that our model learns to generate questions that are less likely to be dismissed by healthcare providers.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will:\n",
    "- Prepare the data for LLM training\n",
    "- Create train/validation/test splits\n",
    "- Format the data for different LLM frameworks\n",
    "- Create evaluation metrics for assessing question quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
